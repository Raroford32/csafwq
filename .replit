modules = ["python-3.11"]

[nix]
channel = "stable-24_05"

[workflows]
runButton = "Project"

[[workflows.workflow]]
name = "Project"
mode = "parallel"
author = "agent"

[[workflows.workflow.tasks]]
task = "workflow.run"
args = "Run Distributed LLM Inference System"

[[workflows.workflow.tasks]]
task = "workflow.run"
args = "Distributed LLM Inference"

[[workflows.workflow.tasks]]
task = "workflow.run"
args = "Check Ray"

[[workflows.workflow]]
name = "Run Distributed LLM Inference System"
author = "agent"

[workflows.workflow.metadata]
agentRequireRestartOnSave = false

[[workflows.workflow.tasks]]
task = "shell.exec"
args = "python main.py"
waitForPort = 8000

[[workflows.workflow]]
name = "Distributed LLM Inference"
author = "agent"

[workflows.workflow.metadata]
agentRequireRestartOnSave = false

[[workflows.workflow.tasks]]
task = "shell.exec"
args = "python main.py"
waitForPort = 8000

[[workflows.workflow]]
name = "Check Ray"
author = "agent"

[workflows.workflow.metadata]
agentRequireRestartOnSave = false

[[workflows.workflow.tasks]]
task = "shell.exec"
args = "python check_ray.py"

[deployment]
run = ["sh", "-c", "python main.py"]

[[ports]]
localPort = 35831
externalPort = 3000

[[ports]]
localPort = 47323
externalPort = 4200
